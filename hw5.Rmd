---
title: "Homework 5"
author: "Lehan Zou"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```




### Problem 0

This "problem" focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files. This was not prepared as a GitHub repo.

```{r load_libraries}
library(tidyverse)
```


## Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides. 


### Problem 2
```{r}
# read in all the dataset
data_files = tibble(
  files = list.files("data/prob2/"),
    path = str_c("data/prob2/", files)
  ) |> 
  mutate(data = map(path, read_csv)) |> 
  unnest()
```

```{r}
data = data_files|>
  mutate(files = str_remove(files, ".csv"))|>
  separate(files, into = c("group", "id"), sep = "_") |> 
  pivot_longer(week_1:week_8,names_to = "week", values_to = "observation",names_prefix = "week_", names_transform = list(week = as.numeric))|>
  select(group, id, week, observation)

head(data)
```

* make spaghetti plots
```{r}
data |> 
  ggplot(aes(x = week, y = observation, group = id, color = id)) +
  geom_line() +
  facet_grid(. ~ group) +
  labs(title = "Observations in Different Groups Over Weeks")
```
From the spaghetti plot, we know that overall the observation increases as time increases, but the values within a certain time are fluctuated. 



### Problem 3
```{r}
t_test =
  function(n, mu, sigma) {
    
      rnorm(n, mu, sigma) |> 
      t.test() |> 
      broom::tidy() |> 
      select(estimate, p.value)
    
  }

sim_df =
  expand_grid(
    mu = 0,
    iter = 1:5000
  ) |> 
  mutate(test_result = map(mu, t_test, n = 30, sigma = 5)) |> 
  unnest(test_result)
```


```{r}
sim2_df = 
  expand_grid(
    mu = c(1:6),
    iter = 1:5000
  ) |> 
  mutate(test_result = map(mu, t_test, n = 30, sigma = 5)) |> 
  unnest(test_result)

sim2_df |> 
  group_by(mu) |> 
  summarise(power = sum(p.value < 0.05) / n()) |> 
  ggplot(aes(x = mu, y = power)) + 
  geom_point() +
  geom_line() +
  labs(title = "Power for Different mu", x = "mu")
```
From above we could observe that the power increases as the effect size increases, eventually it will approaches 1. Also the slope of this curve is decreasing along the true mean.


```{r}
sim2_df |> 
  group_by(mu) |> 
  summarise(mean_estimate = mean(estimate)) |> 
  ggplot(aes(x = mu, y = mean_estimate)) +
  geom_line(aes(color = "all", lty = "all")) +
  geom_line(data = 
              sim2_df |>
              filter(p.value < 0.05) |> 
              group_by(mu) |> 
              summarise(mean_estimate = mean(estimate)),
            aes(color = "rejected", lty = "rejected")) +
  scale_color_manual(name = "", values = c("all" = "blue", "rejected" = "red")) +
  scale_linetype_manual(name = "", values = c("all" = 2, "rejected" = 1)) +
  labs(title = "Average Estimate for Different mu", x = "mu", y = "Average Estimate of mu_hat")
```
The average estimate follows the distribution: $\bar X\sim N(\mu, \frac{\sigma^2}{n}).$ So as $\sigma$ and $n$ are equal, there will be higher possibility to reject the null as $\mu$ increases. The gap between the average estimate and the real value are positive related. The more rejects, the greater the gap. Thus, the gap decreases as $\mu$ increases.

